\documentclass{article}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{amsfonts}
\newtheorem{theorem}{Theorem}

\title{Blendenpik: Randomized Least Squares \\[1ex] \large Project - Computational Linear Algebra, EPFL, Spring 22}
\author{Moritz Waldleben}
\date{May 2022}

\begin{document}

\maketitle

\section{Introduction} \label{intro}
Blendenpik \cite{blendenpik} is a randomized algorithm for a linear
least-squares problem. The method constructs a random projection preconditioner
for an iterative solver (LSQR) and converges in fewer steps than an
un-preconditioned LSQR. Moreover, Blendenpik can beat the standard LAPACK
implementation for very large matrices. 

Instead of the LSQR algorithm used in the paper \cite{blendenpik}, we will use
the minimum residual method (MINRES) with preconditioning. In the following
report, we will discuss the main concepts of Blendenpik and illustrate some
results with a custom MINRES implementation.

\section{Motivation} \label{mot}
Let us look at a large overdetermined system: $A \in \mathbb{R}^{m \times n}, b
\in \mathbb{R}^{m}, m>>n, \text { and } \operatorname{rank}(A)=n$. The
corresponding linear least-squares problem can be written as

\begin{equation} \label{equ_lq}
\min _{x \in \mathbb{R}^{n}}\|A \mathbf{x}-\mathbf{b}\|_{2}
\end{equation}

One might think that the linear system has redundant information. We could thus
try to solve a smaller system and sample $\mathcal{S}$ rows of the matrix to
get an approximated solution.

\begin{equation} \label{equ_lq_red}
x_{\mathcal{R}}=\arg \min _{x}\|A(\mathcal{R},:) x-b(\mathcal{R})\|_{2}
\end{equation}

This reduction technique is rarely implemented, in practice, because of lacking
usefull bounds for the forward error (See \cite{blendenpik} for a detailed
explanation). An alternative way is to construct a preconditioner for the
complete linear system \ref{equ_lq} using randomly selected rows. A reduced
$QR$ factorization of the randomly sampled matrix $A(\mathcal{R},:)$ is a
suitable preconditioner. This property is further discussed in section
\ref{bound}.To implement Blendenpik using MINRES we would need to apply the
iterative solver to the normal equation of the least-squares problem. The
commented implemenation of the complet algorithm can be is shown in appendix
\ref{app_code}. Below an overview of the algorithm is shown.

\begin{algorithm}[htb]
\caption{Blendenpik overview (using MINRES)}
\begin{enumerate}
\item Choose $\mathcal{R}$ of size $r$, a subset of rows of $A$
\item Calculate $A(\mathcal{R},:) = QR$ 
\item Calculate $A(\mathcal{R}, :)A(\mathcal{R}, :)^T$
\item MINRES to $AA^T$ with right and left preconditioner $R^{-T}, R^{-1}$
\end{enumerate}
\end{algorithm}

\section{Coherence of a matrix}
Now we will focus on how to randomly sample rows from a given matrix. To tackle
this, we have to look at the coherence of a matrix $A$. Coherence tells us how
much the solution of the system will depend on a single row of $A$. Formally
coherence can be defined as

\begin{equation} \label{equ_coh}
\mu(A)=\max _{i}\|U(i,:)\|_{2}^{2},
\end{equation}
where $\frac{n}{m} \le \mu(A) \le 1$. 

\bigskip

To illustrate this property, we will look at the coherence of a randomly
generated orthonormal matrices of size $\mathbb{R}^{100 \times 50}$.

\begin{verbatim}
% mean coherence of 1000 random matrices
rng(11);
coherences = [];
for i=1:1000
    U = orth(rand(1000, 50));
    coherence = max(sum(U.^2, 2));
    coherences = [coherences, coherence];
end
disp(mean(coherences))

>> 0.0720
\end{verbatim}

The Matlab script above computes the average coherence number of randomly
generated orthonormal matrices. As comparison, the minimal coherence for matrix
for $U$ is $\mu_{min}(U)= \frac{50}{1000} = 0.05$ from the definition in
\ref{equ_coh}. The proof of the corresponding the bounds can be found in the
\ref{app_coh}.

The result shows that the coherence number is nearly optimal for randomly
generated orthonormal matrices. On the other hand, we can expect this, as all
elements of the matrice are created using a uniform distribution. 

\bigskip

Let's now think of an extreme case where the coherence number is maximal. We
could set all elements of a column of $U$ row to zero except one entry. The
result is a coherence of $\mu(U)=1$. More intuitively, this means that the row with
the nonzero entry has to be sampled to capture the "essence or direction" of
the column with one nonzero entry.

In contrast to a uniform sampled matrix discussed above, general matrices $A$
will have high coherence numbers. But a good preconditener obtained from random sampling
should still resemble $A$ and result in a a low conditioner number for the iterative
solver (see theorem \ref{thm_1}). In summary, we have to control the coherence of $A$ to
obtain a solid preconditioner.

A technique to do so is preprocessing $A$ with randomized row mixing. The rows
of A are blended (therefore the name Blendenpik) and the obtained coherence of
$A_S$ drops. Here several strategies are possible. One of them is multiplying
each row with $+1$ or $-1$. Then a discrete cosine transform (DCT) is applied
to each column. In addition, the first row is multiplied by $\sqrt{2}$ to get
an orthogonal transformation. The result is to reduce the coherence and a solid
preconditioner can still be obtained.


\section{Bound for the condition number} \label{bound}
The obtained preconditioner $R$ from
the discussions in the previous chapter is theoretically just useful if we can
bound the condition number for the preconditioned iterative solver. This
ensures convergences in a few steps. In the case of the MINRES version of
Blendenpik, we would replace $A$ with $AA^T$ and $R$ with $RR^T$. This adaption
will lead to similar bounds as the ones shown below.

The following gives us a desired bound and establishes a link between the
condition and coherence number.

\begin{theorem} \label{thm_1}
Let $A$ be an $m \times n$ full rank matrix, and let $\mathcal{S}$ be a random
sampling operator that samples $r \geq n$ rows from $A$ uniformly. Let $\tau=C
\sqrt{m \mu(A) \log (r) / r}$, where $C$ is some constant defined in the proof.
Assume that $\delta^{-1} \tau<1$. With probability of at least $1-\delta$, the
sampled matrix $\mathcal{S} A$ is full rank, and if $\mathcal{S} A=Q R$ is a
reduced $Q R$ factorization of $\mathcal{S} A$, we have
$$
\kappa\left(A R^{-1}\right) \leq \frac{1+\delta^{-1} \tau}{1-\delta^{-1} \tau}
$$
\end{theorem}

To prove the above we will state and use two theorems which correspond to
theorem 1 in \cite{CUR} and theorem 7 in \cite{randalg}. 

\begin{theorem}{CUR} \label{thm_2}
Suppose that $l, m$, and $n$ are positive integers such that $m \geq l \geq n$.
Suppose further that $A$ is a full-rank $m \times n$ matrix, and that the SVD
of $A$ is
$$
A_{m \times n}=U_{m \times n} \Sigma_{n \times n} V_{n \times n}^{*} .
$$
Suppose in addition that $T$ is an $l \times m$ matrix such that the $l \times
n$ matrix TU has full rank.
Then, there exists an $n \times n$ matrix $P$, and an $l \times n$ matrix $Q$
whose columns are orthonormal, such that
$$
T_{l \times m} A_{m \times n}=Q_{l \times n} P_{n \times n} .
$$
Furthermore, if $P$ is any $n \times n$ matrix, and $Q$ is any $l \times n$
matrix whose columns are orthonormal, such that $P$ and $Q$ satisfy Eq. 27,
then the condition numbers of $A P^{-1}$ and $T U$ are equal.
\end{theorem}

\begin{theorem} \label{thm_3}
 Suppose $A \in \mathbb{R}^{m \times n}, B \in \mathbb{R}^{n \times p}$, and $c
 \leq n$. Construct $C$ and $R$ with Algorithm 6, using the EXPECTED $(c)$
 algorithm. If the sampling probabilities $\left\{p_{i}\right\}_{i=1}^{n}$ used
 by the algorithm are of the form (44) or (45), then
$$
\mathbf{E}\left[\|A B-C R\|_{F}\right] \leq \frac{1}{\sqrt{\beta c}}\|A\|_{F}\|B\|_{F} .
$$
If, in addition, $B=A^{T}$, then
$$
\mathbf{E}\left[\left\|A A^{T}-C C^{T}\right\|_{2}\right] \leq O(1) \sqrt{\frac{\log c}{\beta c}}\|A\|_{F}\|A\|_{2} .
$$
\end{theorem}

Now we will prove the theorem \ref{thm_1}. 

\section{Implementation} \label{impl}

\section{Numerical experiments} \label{num_exp}

\bibliography{refs} 
\bibliographystyle{plain}

\appendix
\section{Matlab implementation Blendenpik} \label{app_code}
\section{Coherence bound proves} \label{app_coh}

\end{document}
